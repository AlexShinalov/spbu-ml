{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "93WqKpqngXqs"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n"
      ],
      "id": "93WqKpqngXqs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JhoLV7ogXqv"
      },
      "source": [
        "## Задание 1\n",
        "\n",
        "Напишите функцию, которая моделирует один нейрон с сигмоидной активацией и реализует вычисление градиента для обновления весов и смещений нейрона. Функция должна принимать список векторов признаков, ассоциированные бинарные метки класса, начальные веса, начальное смещение, скорость обучения и количество эпох. Функция должна обновлять веса и смещение с помощью градиентного спуска (классической версии) на основе функции потерь NLL и возвращать обновленные веса, смещение и список значений NLL для каждой эпохи, округленное до четырех десятичных знаков. Проведите обучение на предоставленном наборе данных из задания 4 (для двух разных лет). Опционально сгенерируйте другие подходящие наборы данных. Опишите ваши результаты. Предоставленная функция будет также протестирована во время защиты ДЗ. Можно использовать только чистый torch (без использования autograd и torch.nn)."
      ],
      "id": "7JhoLV7ogXqv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4HFqaVgXqw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sigmoid(z: torch.Tensor) -> torch.Tensor:\n",
        "    # sigmoid(z) = 1 / (1 + exp(-z))\n",
        "    return torch.where(z >= 0, 1 / (1 + torch.exp(-z)), torch.exp(z) / (1 + torch.exp(z)))\n",
        "\n",
        "\n",
        "def binary_nll(probs: torch.Tensor, y: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
        "    probs = torch.clamp(probs, eps, 1 - eps)\n",
        "    return -(y * torch.log(probs) + (1 - y) * torch.log(1 - probs)).mean()\n"
      ],
      "id": "UT4HFqaVgXqw"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FS7OrMUvsMU3"
      },
      "id": "FS7OrMUvsMU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "features: список списков/тензор размера [N, D]    \n",
        "labels:   список/тензор размера [N], значения {0,1}   \n",
        "initial_weights: список/тензор [D]    \n",
        "nitial_bias: float    \n",
        "learning_rate: float    \n",
        "epochs: int   "
      ],
      "metadata": {
        "id": "CX5EvRaRhm7u"
      },
      "id": "CX5EvRaRhm7u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kEg_BnNgXqz"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_single_sigmoid_neuron_sgd(\n",
        "    features,\n",
        "    labels,\n",
        "    initial_weights,\n",
        "    initial_bias: float,\n",
        "    learning_rate: float,\n",
        "    epochs: int,\n",
        "):\n",
        "\n",
        "\n",
        "    X = torch.tensor(features, dtype=torch.float32)\n",
        "    y = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    w = torch.tensor(initial_weights, dtype=torch.float32).clone()\n",
        "    b = float(initial_bias)\n",
        "\n",
        "    nll_history = []\n",
        "\n",
        "    N = X.shape[0]\n",
        "\n",
        "    for _ in range(int(epochs)):\n",
        "        # forward\n",
        "        z = X @ w + b\n",
        "        p = sigmoid(z)\n",
        "        loss = binary_nll(p, y)\n",
        "\n",
        "        # grad_w = X^T (p - y) / N, grad_b = mean(p - y)\n",
        "        err = (p - y)\n",
        "        grad_w = (X.T @ err) / N\n",
        "        grad_b = err.mean().item()\n",
        "\n",
        "        # обновялем\n",
        "        w = w - learning_rate * grad_w\n",
        "        b = b - learning_rate * grad_b\n",
        "\n",
        "        nll_history.append(round(loss.item(), 4))\n",
        "\n",
        "    return w.tolist(), float(b), nll_history\n"
      ],
      "id": "9kEg_BnNgXqz"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xihO_wYFiXga"
      },
      "id": "xihO_wYFiXga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDsSat5LgXq-"
      },
      "source": [
        "## Задание 3\n",
        "\n",
        "Реализуйте один из оптимизаторов на выбор. Придумайте и напишите тесты для проверки выбранного оптимизатора. Проведите обучение нейрона из первого задания с использованием оптимизатора, а не ванильного градиентного спуска. Также опишите идею алгоритма (+1 балл)."
      ],
      "id": "RDsSat5LgXq-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHVQD5WRgXq-"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "class Adagrad:\n",
        "    def __init__(self, lr: float = 0.1, eps: float = 1e-10):\n",
        "        self.lr = float(lr)\n",
        "        self.eps = float(eps)\n",
        "        self.accum = None\n",
        "        # будет tensor той же формы, что и параметр\n",
        "\n",
        "    def step(self, param: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
        "        # Возвращает обновленный параметр (не in-place)\n",
        "        if self.accum is None:\n",
        "            self.accum = torch.zeros_like(param)\n",
        "\n",
        "        self.accum = self.accum + grad * grad\n",
        "        adjusted_lr = self.lr / (torch.sqrt(self.accum) + self.eps)\n",
        "        return param - adjusted_lr * grad\n"
      ],
      "id": "ZHVQD5WRgXq-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Идея алгоритма\n",
        "Адаптивно подбирает шаг обучения для каждого параметра отдельно на основе истории его градиентов. Параметры с часто наблюдаются большими градиентами получают меньший эффективный шаг, а параметры с малыми градиентами больший шаг.\n",
        "Шаг монотонно убывает со временем из-за этого обучение может преждевременно тухнуть."
      ],
      "metadata": {
        "id": "vIkTGeDtegqE"
      },
      "id": "vIkTGeDtegqE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-35Vwd7jgXq_"
      },
      "source": [
        "### Тесты для Adagrad\n",
        "Тестируем:\n",
        "1) Правильность первого шага (accum = g^2, масштабирование)\n",
        "2) Обработка разных форм (вектор параметров)\n"
      ],
      "id": "-35Vwd7jgXq_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t04Atgz2gXrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501e3a4d-980d-455e-b4ec-344002b78d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adagrad tests: OK\n"
          ]
        }
      ],
      "source": [
        "def test_adagrad_single_step():\n",
        "    opt = Adagrad(lr=1.0, eps=0.0)\n",
        "    p0 = torch.tensor([1.0])\n",
        "    g = torch.tensor([2.0])\n",
        "    # accum = 4, sqrt=2, lr/sqrt=0.5 => p1 = 1 - 0.5*2 = 0\n",
        "    p1 = opt.step(p0, g)\n",
        "    assert torch.allclose(p1, torch.tensor([0.0])), (p1, opt.accum)\n",
        "\n",
        "\n",
        "def test_adagrad_vector_params():\n",
        "    opt = Adagrad(lr=1.0, eps=0.0)\n",
        "    p0 = torch.tensor([1.0, 2.0])\n",
        "    g = torch.tensor([1.0, 2.0])\n",
        "    p1 = opt.step(p0, g)\n",
        "    # coord0: accum=1 sqrt=1 lr=1 => p=0\n",
        "    # coord1: accum=4 sqrt=2 lr=0.5 => p=2-0.5*2=1\n",
        "    assert torch.allclose(p1, torch.tensor([0.0, 1.0])), (p1, opt.accum)\n",
        "\n",
        "test_adagrad_single_step()\n",
        "test_adagrad_vector_params()\n",
        "print(\"Adagrad tests: OK\")\n"
      ],
      "id": "t04Atgz2gXrB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGG_0ZGNgXrB"
      },
      "source": [
        "### Обучение нейрона из задания 1 с Adagrad\n",
        "Вместо vanilla GD используем оптимизатор для `w` и отдельный оптимизатор для `b` (скаляр).\n"
      ],
      "id": "IGG_0ZGNgXrB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E28hERdgXrB"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_single_sigmoid_neuron_adagrad(\n",
        "    features,\n",
        "    labels,\n",
        "    initial_weights,\n",
        "    initial_bias: float,\n",
        "    learning_rate: float,\n",
        "    epochs: int,\n",
        "    eps: float = 1e-10,\n",
        "):\n",
        "    X = torch.tensor(features, dtype=torch.float32)\n",
        "    y = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    w = torch.tensor(initial_weights, dtype=torch.float32).clone()\n",
        "    b = torch.tensor([float(initial_bias)], dtype=torch.float32)\n",
        "\n",
        "    opt_w = Adagrad(lr=learning_rate, eps=eps)\n",
        "    opt_b = Adagrad(lr=learning_rate, eps=eps)\n",
        "\n",
        "    N = X.shape[0]\n",
        "    nll_history = []\n",
        "\n",
        "    for _ in range(int(epochs)):\n",
        "        z = X @ w + b.item()\n",
        "        p = sigmoid(z)\n",
        "        loss = binary_nll(p, y)\n",
        "\n",
        "        err = (p - y)\n",
        "        grad_w = (X.T @ err) / N\n",
        "        grad_b = torch.tensor([err.mean().item()], dtype=torch.float32)  # [1]\n",
        "\n",
        "        w = opt_w.step(w, grad_w)\n",
        "        b = opt_b.step(b, grad_b)\n",
        "\n",
        "        nll_history.append(round(loss.item(), 4))\n",
        "\n",
        "    return w.tolist(), float(b.item()), nll_history\n"
      ],
      "id": "-E28hERdgXrB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тест"
      ],
      "metadata": {
        "id": "MHBqKozHiGa7"
      },
      "id": "MHBqKozHiGa7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oc_NcBgXrE",
        "outputId": "9e185412-688f-4b47-c927-66c104e1aac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merged shape: (14000, 92)\n",
            "year unique: 79 range: 1922 - 2011\n",
            "top years:\n",
            " year\n",
            "2007    1102\n",
            "2006    1031\n",
            "2005    1001\n",
            "2008     951\n",
            "2009     835\n",
            "2004     792\n",
            "2003     703\n",
            "2002     581\n",
            "2001     580\n",
            "2000     539\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "x_pat = \"/content/train_x.csv\"\n",
        "y_path = \"/content/train_y.csv\"\n",
        "\n",
        "dfx = pd.read_csv(x_pa)\n",
        "dfy = pd.read_csv(y_path)\n",
        "\n",
        "if \"Unnamed: 0\" in dfx.columns and \"Unnamed: 0\" in dfy.columns:\n",
        "    df = dfx.merge(dfy, on=\"Unnamed: 0\", how=\"inner\")\n",
        "else:\n",
        "    df = dfx.copy()\n",
        "    df[\"year\"] = dfy[\"year\"].values\n",
        "\n",
        "print(\"merged shape:\", df.shape)\n",
        "print(\"year unique:\", df[\"year\"].nunique(), \"range:\", int(df[\"year\"].min()), \"-\", int(df[\"year\"].max()))\n",
        "print(\"top years:\", df[\"year\"].value_counts().head(10))\n"
      ],
      "id": "a9oc_NcBgXrE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bDfkK3JgXrF",
        "outputId": "6acc5229-3eed-4607-fc1e-c2b0d9795827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using years: 2007 (label=1) vs 2006 (label=0)\n",
            "Binary dataset: 2133 samples, 90 features; positive rate: 0.516643225503985\n"
          ]
        }
      ],
      "source": [
        "def make_binary_year_dataset(df: pd.DataFrame, year_pos: int, year_neg: int, normalize: bool = True):\n",
        "    # 2 года\n",
        "    sub = df[(df[\"year\"] == year_pos) | (df[\"year\"] == year_neg)].copy()\n",
        "\n",
        "    # Признаки: все колонки которые выглядят как числа\n",
        "    drop_cols = set([\"year\"])\n",
        "\n",
        "    feature_cols = [c for c in sub.columns if c not in drop_cols]\n",
        "\n",
        "    X = sub[feature_cols].astype(\"float32\").values\n",
        "    y = (sub[\"year\"].astype(\"int64\").values == int(year_pos)).astype(\"int64\")  # 1 для year_pos, 0 для year_neg\n",
        "\n",
        "    if normalize:\n",
        "        # стандартизация по выборке\n",
        "        mu = X.mean(axis=0, keepdims=True)\n",
        "        std = X.std(axis=0, keepdims=True)\n",
        "        std[std < 1e-6] = 1.0\n",
        "        X = (X - mu) / std\n",
        "\n",
        "    return X.tolist(), y.tolist(), feature_cols, sub\n",
        "\n",
        "# самые частые года\n",
        "top2 = df[\"year\"].value_counts().head(2).index.tolist()\n",
        "YEAR_POS, YEAR_NEG = int(top2[0]), int(top2[1])\n",
        "print(\"Using years:\", YEAR_POS, \"(label=1) vs\", YEAR_NEG, \"(label=0)\")\n",
        "\n",
        "X_bin, y_bin, feature_cols, sub_df = make_binary_year_dataset(df, YEAR_POS, YEAR_NEG, normalize=True)\n",
        "print(\"Binary dataset:\", len(X_bin), \"samples,\", len(feature_cols), \"features; positive rate:\", sum(y_bin)/len(y_bin))\n"
      ],
      "id": "_bDfkK3JgXrF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMf4sttngXrG"
      },
      "source": [
        "### Обучение (vanilla GD из задания 1)"
      ],
      "id": "LMf4sttngXrG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AC2OSGpgXrH",
        "outputId": "74e0caa5-b7d5-4e91-91fd-3b36ba9a2c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final NLL (GD): 0.6763\n",
            "First 10 NLL: [0.6931, 0.6923, 0.6915, 0.6909, 0.6903, 0.6898, 0.6893, 0.6889, 0.6884, 0.688]\n"
          ]
        }
      ],
      "source": [
        "D = len(feature_cols)\n",
        "init_w = [0.0] * D\n",
        "init_b = 0.0\n",
        "\n",
        "w_gd, b_gd, nll_gd = train_single_sigmoid_neuron_sgd(\n",
        "    X_bin, y_bin,\n",
        "    initial_weights=init_w,\n",
        "    initial_bias=init_b,\n",
        "    learning_rate=0.1,\n",
        "    epochs=300\n",
        ")\n",
        "\n",
        "print(\"Final NLL (GD):\", nll_gd[-1])\n",
        "print(\"First 10 NLL:\", nll_gd[:10])\n"
      ],
      "id": "1AC2OSGpgXrH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWZzBcHZgXrH"
      },
      "source": [
        "### Обучение (Adagrad из задания 3)"
      ],
      "id": "DWZzBcHZgXrH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAJbBj_TgXrH",
        "outputId": "02d413ed-b848-4bb9-9c37-284beecdcaa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final NLL (Adagrad): 0.6765\n",
            "First 10 NLL: [0.6931, nan, nan, nan, 1.4571, nan, inf, nan, 1.0312, nan]\n"
          ]
        }
      ],
      "source": [
        "w_ada, b_ada, nll_ada = train_single_sigmoid_neuron_adagrad(\n",
        "    X_bin, y_bin,\n",
        "    initial_weights=init_w,\n",
        "    initial_bias=init_b,\n",
        "    learning_rate=0.5,\n",
        "    epochs=300,\n",
        "    eps=1e-10\n",
        ")\n",
        "\n",
        "print(\"Final NLL (Adagrad):\", nll_ada[-1])\n",
        "print(\"First 10 NLL:\", nll_ada[:10])\n"
      ],
      "id": "IAJbBj_TgXrH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дополнительное\n",
        "(10 баллов) Реализуйте базовые функции autograd. Можете вдохновиться видео от Andrej Karpathy. Напишите класс, аналогичный предоставленному классу 'Element', который реализует основные операции autograd: сложение, умножение и активацию ReLU. Класс должен обрабатывать скалярные объекты и правильно вычислять градиенты для этих операций посредством автоматического дифференцирования. Плюсом будет набор предоставленных тестов, оценивающих правильность вычислений. Большим плюсом будет, если тесты будут написаны с помощью unittest. Можно использовать только чистый torch (без использования autograd и torch.nn). За каждую нереализованную операцию будет вычитаться 3 балла. Пример: a = Node(2) b = Node(-3) c = Node(10) d = a + b * c e = d.relu() e.backward() print(a, b, c, d, e) Output:  Node(data=2, grad=0)  Node(data=-3, grad=10)  Node(data=10, grad=-3)  Node(data=-28, grad=1)  Node(data=0, grad=1)"
      ],
      "metadata": {
        "id": "omtfbUJub0lZ"
      },
      "id": "omtfbUJub0lZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Как должно работать\n",
        "Запускает обратное распространение градиента по графу.\n",
        "        1) Строим топологический порядок узлов (DFS).   \n",
        "        2) Инициализируем grad у текущего узла как 1 (dL/dL).   \n",
        "        3) Идём по узлам в обратном топологическом порядке и вызываем _backward ().   "
      ],
      "metadata": {
        "id": "XTGXVifhfS8Z"
      },
      "id": "XTGXVifhfS8Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, data, _children=(), _op=''):\n",
        "        # Приводим к float (можно передавать int/float/torch scalar)\n",
        "        if isinstance(data, torch.Tensor):\n",
        "            # Допускаем torch scalar (0-dim), но без autograd\n",
        "            self.data = float(data.item())\n",
        "        else:\n",
        "            self.data = float(data)\n",
        "\n",
        "        self.grad = 0.0\n",
        "        self._backward = lambda: None\n",
        "        self._prev = set(_children)\n",
        "        self._op = _op\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node(data={self.data:g}, grad={self.grad:g})\"\n",
        "\n",
        "    # приводим число/Node к Node\n",
        "    @staticmethod\n",
        "    def _to_node(x):\n",
        "        return x if isinstance(x, Node) else Node(x)\n",
        "\n",
        "    # сложение\n",
        "    def __add__(self, other):\n",
        "        other = Node._to_node(other)\n",
        "        out = Node(self.data + other.data, (self, other), _op='+')\n",
        "\n",
        "        def _backward():\n",
        "            # d(out)/d(self)=1, d(out)/d(other)=1\n",
        "            self.grad += 1.0 * out.grad\n",
        "            other.grad += 1.0 * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        # other + self\n",
        "        return Node._to_node(other) + self\n",
        "\n",
        "    # умножение\n",
        "    def __mul__(self, other):\n",
        "        other = Node._to_node(other)\n",
        "        out = Node(self.data * other.data, (self, other), _op='*')\n",
        "\n",
        "        def _backward():\n",
        "            # out = self * other\n",
        "            # d(out)/d(self)=other.data, d(out)/d(other)=self.data\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def __rmul__(self, other):\n",
        "        # other * self\n",
        "        return Node._to_node(other) * self\n",
        "\n",
        "    def relu(self):\n",
        "        out = Node(self.data if self.data > 0 else 0.0, (self,), _op='relu')\n",
        "\n",
        "        def _backward():\n",
        "            # dReLU(x)/dx = 1, если x>0, иначе 0\n",
        "            self.grad += (1.0 if self.data > 0 else 0.0) * out.grad\n",
        "\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "\n",
        "        build_topo(self)\n",
        "\n",
        "        # Обнуляем грады во всём графе (чтобы повторные backward были корректны)\n",
        "        for v in topo:\n",
        "            v.grad = 0.0\n",
        "\n",
        "        # dL/dL = 1\n",
        "        self.grad = 1.0\n",
        "\n",
        "        # В обратном порядке вызываем локальные backward\n",
        "        for v in reversed(topo):\n",
        "            v._backward()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "paCwlcFdb3lV"
      },
      "id": "paCwlcFdb3lV",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = Node(2)\n",
        "b = Node(-3)\n",
        "c = Node(10)\n",
        "d = a + b * c\n",
        "e = d.relu()\n",
        "e.backward()\n",
        "print(a, b, c, d, e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRLP-hgCc6Dk",
        "outputId": "2e476eb5-6ad0-4a8a-baf9-8a6f1fe4abe8"
      },
      "id": "yRLP-hgCc6Dk",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CASE 1 (ReLU off):\n",
            "a b c d e = Node(data=2, grad=0) Node(data=-3, grad=0) Node(data=10, grad=0) Node(data=-28, grad=0) Node(data=0, grad=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = Node(2)\n",
        "b = Node(-3)\n",
        "c = Node(-10)\n",
        "d = a + b * c\n",
        "e = d.relu()\n",
        "e.backward()\n",
        "print(a, b, c, d, e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ1W5N0pdEGA",
        "outputId": "89e51692-b6d5-48fb-894f-3f2384689bb2"
      },
      "id": "hQ1W5N0pdEGA",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node(data=2, grad=1) Node(data=-3, grad=-10) Node(data=-10, grad=-3) Node(data=32, grad=1) Node(data=32, grad=1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}