{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "93WqKpqngXqs"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n"
      ],
      "id": "93WqKpqngXqs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JhoLV7ogXqv"
      },
      "source": [
        "## Задание 1\n",
        "\n",
        "Напишите функцию, которая моделирует один нейрон с сигмоидной активацией и реализует вычисление градиента для обновления весов и смещений нейрона. Функция должна принимать список векторов признаков, ассоциированные бинарные метки класса, начальные веса, начальное смещение, скорость обучения и количество эпох. Функция должна обновлять веса и смещение с помощью градиентного спуска (классической версии) на основе функции потерь NLL и возвращать обновленные веса, смещение и список значений NLL для каждой эпохи, округленное до четырех десятичных знаков. Проведите обучение на предоставленном наборе данных из задания 4 (для двух разных лет). Опционально сгенерируйте другие подходящие наборы данных. Опишите ваши результаты. Предоставленная функция будет также протестирована во время защиты ДЗ. Можно использовать только чистый torch (без использования autograd и torch.nn)."
      ],
      "id": "7JhoLV7ogXqv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT4HFqaVgXqw"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def sigmoid(z: torch.Tensor) -> torch.Tensor:\n",
        "    # sigmoid(z) = 1 / (1 + exp(-z))\n",
        "    return torch.where(z >= 0, 1 / (1 + torch.exp(-z)), torch.exp(z) / (1 + torch.exp(z)))\n",
        "\n",
        "\n",
        "def binary_nll(probs: torch.Tensor, y: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
        "    probs = torch.clamp(probs, eps, 1 - eps)\n",
        "    return -(y * torch.log(probs) + (1 - y) * torch.log(1 - probs)).mean()\n"
      ],
      "id": "UT4HFqaVgXqw"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FS7OrMUvsMU3"
      },
      "id": "FS7OrMUvsMU3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "features: список списков/тензор размера [N, D]    \n",
        "labels:   список/тензор размера [N], значения {0,1}   \n",
        "initial_weights: список/тензор [D]    \n",
        "nitial_bias: float    \n",
        "learning_rate: float    \n",
        "epochs: int   "
      ],
      "metadata": {
        "id": "CX5EvRaRhm7u"
      },
      "id": "CX5EvRaRhm7u"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kEg_BnNgXqz"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_single_sigmoid_neuron_sgd(\n",
        "    features,\n",
        "    labels,\n",
        "    initial_weights,\n",
        "    initial_bias: float,\n",
        "    learning_rate: float,\n",
        "    epochs: int,\n",
        "):\n",
        "\n",
        "\n",
        "    X = torch.tensor(features, dtype=torch.float32)\n",
        "    y = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    w = torch.tensor(initial_weights, dtype=torch.float32).clone()\n",
        "    b = float(initial_bias)\n",
        "\n",
        "    nll_history = []\n",
        "\n",
        "    N = X.shape[0]\n",
        "\n",
        "    for _ in range(int(epochs)):\n",
        "        # forward\n",
        "        z = X @ w + b\n",
        "        p = sigmoid(z)\n",
        "        loss = binary_nll(p, y)\n",
        "\n",
        "        # grad_w = X^T (p - y) / N, grad_b = mean(p - y)\n",
        "        err = (p - y)\n",
        "        grad_w = (X.T @ err) / N\n",
        "        grad_b = err.mean().item()\n",
        "\n",
        "        # обновялем\n",
        "        w = w - learning_rate * grad_w\n",
        "        b = b - learning_rate * grad_b\n",
        "\n",
        "        nll_history.append(round(loss.item(), 4))\n",
        "\n",
        "    return w.tolist(), float(b), nll_history\n"
      ],
      "id": "9kEg_BnNgXqz"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xihO_wYFiXga"
      },
      "id": "xihO_wYFiXga",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDsSat5LgXq-"
      },
      "source": [
        "## Задание 3\n",
        "\n",
        "Реализуйте один из оптимизаторов на выбор. Придумайте и напишите тесты для проверки выбранного оптимизатора. Проведите обучение нейрона из первого задания с использованием оптимизатора, а не ванильного градиентного спуска. Также опишите идею алгоритма (+1 балл)."
      ],
      "id": "RDsSat5LgXq-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHVQD5WRgXq-"
      },
      "execution_count": 12,
      "outputs": [],
      "source": [
        "class Adagrad:\n",
        "    def __init__(self, lr: float = 0.1, eps: float = 1e-10):\n",
        "        self.lr = float(lr)\n",
        "        self.eps = float(eps)\n",
        "        self.accum = None\n",
        "        # будет tensor той же формы, что и параметр\n",
        "\n",
        "    def step(self, param: torch.Tensor, grad: torch.Tensor) -> torch.Tensor:\n",
        "        # Возвращает обновленный параметр (не in-place)\n",
        "        if self.accum is None:\n",
        "            self.accum = torch.zeros_like(param)\n",
        "\n",
        "        self.accum = self.accum + grad * grad\n",
        "        adjusted_lr = self.lr / (torch.sqrt(self.accum) + self.eps)\n",
        "        return param - adjusted_lr * grad\n"
      ],
      "id": "ZHVQD5WRgXq-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-35Vwd7jgXq_"
      },
      "source": [
        "### Тесты для Adagrad\n",
        "Тестируем:\n",
        "1) Правильность первого шага (accum = g^2, масштабирование)\n",
        "2) Обработка разных форм (вектор параметров)\n"
      ],
      "id": "-35Vwd7jgXq_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t04Atgz2gXrB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "501e3a4d-980d-455e-b4ec-344002b78d52"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adagrad tests: OK\n"
          ]
        }
      ],
      "source": [
        "def test_adagrad_single_step():\n",
        "    opt = Adagrad(lr=1.0, eps=0.0)\n",
        "    p0 = torch.tensor([1.0])\n",
        "    g = torch.tensor([2.0])\n",
        "    # accum = 4, sqrt=2, lr/sqrt=0.5 => p1 = 1 - 0.5*2 = 0\n",
        "    p1 = opt.step(p0, g)\n",
        "    assert torch.allclose(p1, torch.tensor([0.0])), (p1, opt.accum)\n",
        "\n",
        "\n",
        "def test_adagrad_vector_params():\n",
        "    opt = Adagrad(lr=1.0, eps=0.0)\n",
        "    p0 = torch.tensor([1.0, 2.0])\n",
        "    g = torch.tensor([1.0, 2.0])\n",
        "    p1 = opt.step(p0, g)\n",
        "    # coord0: accum=1 sqrt=1 lr=1 => p=0\n",
        "    # coord1: accum=4 sqrt=2 lr=0.5 => p=2-0.5*2=1\n",
        "    assert torch.allclose(p1, torch.tensor([0.0, 1.0])), (p1, opt.accum)\n",
        "\n",
        "test_adagrad_single_step()\n",
        "test_adagrad_vector_params()\n",
        "print(\"Adagrad tests: OK\")\n"
      ],
      "id": "t04Atgz2gXrB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGG_0ZGNgXrB"
      },
      "source": [
        "### Обучение нейрона из задания 1 с Adagrad\n",
        "Вместо vanilla GD используем оптимизатор для `w` и отдельный оптимизатор для `b` (скаляр).\n"
      ],
      "id": "IGG_0ZGNgXrB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E28hERdgXrB"
      },
      "execution_count": 14,
      "outputs": [],
      "source": [
        "def train_single_sigmoid_neuron_adagrad(\n",
        "    features,\n",
        "    labels,\n",
        "    initial_weights,\n",
        "    initial_bias: float,\n",
        "    learning_rate: float,\n",
        "    epochs: int,\n",
        "    eps: float = 1e-10,\n",
        "):\n",
        "    X = torch.tensor(features, dtype=torch.float32)\n",
        "    y = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    w = torch.tensor(initial_weights, dtype=torch.float32).clone()\n",
        "    b = torch.tensor([float(initial_bias)], dtype=torch.float32)\n",
        "\n",
        "    opt_w = Adagrad(lr=learning_rate, eps=eps)\n",
        "    opt_b = Adagrad(lr=learning_rate, eps=eps)\n",
        "\n",
        "    N = X.shape[0]\n",
        "    nll_history = []\n",
        "\n",
        "    for _ in range(int(epochs)):\n",
        "        z = X @ w + b.item()\n",
        "        p = sigmoid(z)\n",
        "        loss = binary_nll(p, y)\n",
        "\n",
        "        err = (p - y)\n",
        "        grad_w = (X.T @ err) / N\n",
        "        grad_b = torch.tensor([err.mean().item()], dtype=torch.float32)  # [1]\n",
        "\n",
        "        w = opt_w.step(w, grad_w)\n",
        "        b = opt_b.step(b, grad_b)\n",
        "\n",
        "        nll_history.append(round(loss.item(), 4))\n",
        "\n",
        "    return w.tolist(), float(b.item()), nll_history\n"
      ],
      "id": "-E28hERdgXrB"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MHBqKozHiGa7"
      },
      "id": "MHBqKozHiGa7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9oc_NcBgXrE",
        "outputId": "9e185412-688f-4b47-c927-66c104e1aac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merged shape: (14000, 92)\n",
            "year unique: 79 range: 1922 - 2011\n",
            "top years:\n",
            " year\n",
            "2007    1102\n",
            "2006    1031\n",
            "2005    1001\n",
            "2008     951\n",
            "2009     835\n",
            "2004     792\n",
            "2003     703\n",
            "2002     581\n",
            "2001     580\n",
            "2000     539\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "x_pat = \"/content/train_x.csv\"\n",
        "y_path = \"/content/train_y.csv\"\n",
        "\n",
        "dfx = pd.read_csv(x_pa)\n",
        "dfy = pd.read_csv(y_path)\n",
        "\n",
        "if \"Unnamed: 0\" in dfx.columns and \"Unnamed: 0\" in dfy.columns:\n",
        "    df = dfx.merge(dfy, on=\"Unnamed: 0\", how=\"inner\")\n",
        "else:\n",
        "    df = dfx.copy()\n",
        "    df[\"year\"] = dfy[\"year\"].values\n",
        "\n",
        "print(\"merged shape:\", df.shape)\n",
        "print(\"year unique:\", df[\"year\"].nunique(), \"range:\", int(df[\"year\"].min()), \"-\", int(df[\"year\"].max()))\n",
        "print(\"top years:\", df[\"year\"].value_counts().head(10))\n"
      ],
      "id": "a9oc_NcBgXrE"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bDfkK3JgXrF",
        "outputId": "6acc5229-3eed-4607-fc1e-c2b0d9795827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using years: 2007 (label=1) vs 2006 (label=0)\n",
            "Binary dataset: 2133 samples, 90 features; positive rate: 0.516643225503985\n"
          ]
        }
      ],
      "source": [
        "def make_binary_year_dataset(df: pd.DataFrame, year_pos: int, year_neg: int, normalize: bool = True):\n",
        "    # 2 года\n",
        "    sub = df[(df[\"year\"] == year_pos) | (df[\"year\"] == year_neg)].copy()\n",
        "\n",
        "    # Признаки: все колонки которые выглядят как числа\n",
        "    drop_cols = set([\"year\"])\n",
        "\n",
        "    feature_cols = [c for c in sub.columns if c not in drop_cols]\n",
        "\n",
        "    X = sub[feature_cols].astype(\"float32\").values\n",
        "    y = (sub[\"year\"].astype(\"int64\").values == int(year_pos)).astype(\"int64\")  # 1 для year_pos, 0 для year_neg\n",
        "\n",
        "    if normalize:\n",
        "        # стандартизация по выборке\n",
        "        mu = X.mean(axis=0, keepdims=True)\n",
        "        std = X.std(axis=0, keepdims=True)\n",
        "        std[std < 1e-6] = 1.0\n",
        "        X = (X - mu) / std\n",
        "\n",
        "    return X.tolist(), y.tolist(), feature_cols, sub\n",
        "\n",
        "# самые частые года\n",
        "top2 = df[\"year\"].value_counts().head(2).index.tolist()\n",
        "YEAR_POS, YEAR_NEG = int(top2[0]), int(top2[1])\n",
        "print(\"Using years:\", YEAR_POS, \"(label=1) vs\", YEAR_NEG, \"(label=0)\")\n",
        "\n",
        "X_bin, y_bin, feature_cols, sub_df = make_binary_year_dataset(df, YEAR_POS, YEAR_NEG, normalize=True)\n",
        "print(\"Binary dataset:\", len(X_bin), \"samples,\", len(feature_cols), \"features; positive rate:\", sum(y_bin)/len(y_bin))\n"
      ],
      "id": "_bDfkK3JgXrF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMf4sttngXrG"
      },
      "source": [
        "### Обучение (vanilla GD из задания 1)"
      ],
      "id": "LMf4sttngXrG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AC2OSGpgXrH",
        "outputId": "74e0caa5-b7d5-4e91-91fd-3b36ba9a2c6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final NLL (GD): 0.6763\n",
            "First 10 NLL: [0.6931, 0.6923, 0.6915, 0.6909, 0.6903, 0.6898, 0.6893, 0.6889, 0.6884, 0.688]\n"
          ]
        }
      ],
      "source": [
        "D = len(feature_cols)\n",
        "init_w = [0.0] * D\n",
        "init_b = 0.0\n",
        "\n",
        "w_gd, b_gd, nll_gd = train_single_sigmoid_neuron_sgd(\n",
        "    X_bin, y_bin,\n",
        "    initial_weights=init_w,\n",
        "    initial_bias=init_b,\n",
        "    learning_rate=0.1,\n",
        "    epochs=300\n",
        ")\n",
        "\n",
        "print(\"Final NLL (GD):\", nll_gd[-1])\n",
        "print(\"First 10 NLL:\", nll_gd[:10])\n"
      ],
      "id": "1AC2OSGpgXrH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWZzBcHZgXrH"
      },
      "source": [
        "### Обучение (Adagrad из задания 3)"
      ],
      "id": "DWZzBcHZgXrH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAJbBj_TgXrH",
        "outputId": "02d413ed-b848-4bb9-9c37-284beecdcaa2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final NLL (Adagrad): 0.6765\n",
            "First 10 NLL: [0.6931, nan, nan, nan, 1.4571, nan, inf, nan, 1.0312, nan]\n"
          ]
        }
      ],
      "source": [
        "w_ada, b_ada, nll_ada = train_single_sigmoid_neuron_adagrad(\n",
        "    X_bin, y_bin,\n",
        "    initial_weights=init_w,\n",
        "    initial_bias=init_b,\n",
        "    learning_rate=0.5,\n",
        "    epochs=300,\n",
        "    eps=1e-10\n",
        ")\n",
        "\n",
        "print(\"Final NLL (Adagrad):\", nll_ada[-1])\n",
        "print(\"First 10 NLL:\", nll_ada[:10])\n"
      ],
      "id": "IAJbBj_TgXrH"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}